{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Analysis for Guayas Region // Week 1 // Data Preperation"
      ],
      "metadata": {
        "id": "YIqxGpxUPLFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Import Libraries"
      ],
      "metadata": {
        "id": "mhf9oQU6PPOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GIFFXqhBjn9t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # data manupulation\n",
        "import numpy as np # numeric operations\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import seaborn as sns # data visualization\n",
        "import os\n",
        "from google.colab import drive # Import drive folder to google colab\n",
        "from pathlib import Path\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Config and Loading DataFrames"
      ],
      "metadata": {
        "id": "qcvqQOApGgXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Connect Google Drive"
      ],
      "metadata": {
        "id": "edbTatc38ao-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lmbzaKZVEh_",
        "outputId": "3624129c-c120-4469-b7fa-49f7a50ad324"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Set Data Path"
      ],
      "metadata": {
        "id": "TMkUfe0f8ngI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = Path(\"/content/drive/MyDrive/Time Series Analysis /favorita-grocery-sales-forecasting\")"
      ],
      "metadata": {
        "id": "oN0a4hDv8mDK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VtN3aFf_awC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Data Loading"
      ],
      "metadata": {
        "id": "6frjnJHb8_SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load all supporting CSV files except train.csv.\n",
        "df_items = pd.read_csv(DATA_PATH / \"items.csv\")\n",
        "df_stores = pd.read_csv(DATA_PATH / \"stores.csv\")\n",
        "df_oil = pd.read_csv(DATA_PATH / \"oil.csv\")\n",
        "df_holidays_events = pd.read_csv(DATA_PATH / \"holidays_events.csv\")\n",
        "df_transactions = pd.read_csv(DATA_PATH / \"transactions.csv\")"
      ],
      "metadata": {
        "id": "qCvsPmxhkObr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ###Load train.csv in chunks and filter only for given store_ids.\n",
        "guayas_stores = df_stores.loc[df_stores[\"state\"] == \"Guayas\", \"store_nbr\"].unique()\n",
        "chunks = []\n",
        "for chunk in pd.read_csv(\n",
        "    DATA_PATH / \"train.csv\",\n",
        "    parse_dates=[\"date\"],\n",
        "    chunksize=1_000_000,):\n",
        "    chunks.append(chunk[chunk[\"store_nbr\"].isin(guayas_stores)])\n",
        "\n",
        "df_train = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "del chunks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw5ffTH19OJv",
        "outputId": "9a0742b3-7a71-4f04-d967-ebd4c58b0d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-358988839.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in pd.read_csv(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "UrwungCFVCLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Reduce Data by Top 3 Product Families"
      ],
      "metadata": {
        "id": "vudWQXJcPdGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of items per family and select the top 3 families\n",
        "item_counts = df_items.groupby(\"family\")[\"item_nbr\"].nunique().sort_values(ascending=False)\n",
        "top_families = item_counts.head(3)\n",
        "\n",
        "\n",
        "# Get only items that belong to the top families\n",
        "items_top = df_items[df_items[\"family\"].isin(top_families.index)]  # use .index (family names)\n",
        "\n",
        "# Keep only training data with those items\n",
        "df_train = df_train[df_train['item_nbr'].isin(items_top[\"item_nbr\"])]\n",
        "\n"
      ],
      "metadata": {
        "id": "dNRDlwxpTzhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 3 families\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(top_families.index, top_families.values, color='skyblue', edgecolor='black')  # Use .index and .values\n",
        "plt.title(\"Top 3 Families by Number of Items\", fontsize=20, fontweight='bold')\n",
        "plt.xlabel(\"Family\", fontsize=16)\n",
        "plt.ylabel(\"Number of Items\", fontsize=16)\n",
        "plt.xticks(fontsize=14, rotation=45)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-wGiGCzT8CMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "GkvQp6NQqi8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Down-sample for speedy experiments"
      ],
      "metadata": {
        "id": "YiCdj3qEQBLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "S0azjfWMTXhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.sample(n=2000000, random_state=42)\n",
        "\n",
        "df_train.shape"
      ],
      "metadata": {
        "id": "BICsTst9QBCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Save a .Pickle file"
      ],
      "metadata": {
        "id": "Njr_hjE5QQzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save DataFrame to drive folder as .pickle file\n",
        "save_path = '/content/drive/MyDrive/Time Series Analysis /favorita-grocery-sales-forecasting/df_train_filtered.pkl'\n",
        "\n",
        "df_train.to_pickle(save_path)\n",
        "\n",
        "df_train.shape"
      ],
      "metadata": {
        "id": "y2ykHc2KlTDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load df_train dataframe from pickle.file to reload without rerunning the chunk loop again\n",
        "df_train = pd.read_pickle(DATA_PATH / \"df_train_filtered.pkl\")"
      ],
      "metadata": {
        "id": "Nd_w_zrFVPi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "b3uvxropUEdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Missing Value Check"
      ],
      "metadata": {
        "id": "Vd87m7wIPpXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {\n",
        "    \"stores\": df_stores,\n",
        "    \"items\": df_items,\n",
        "    \"transactions\": df_transactions,\n",
        "    \"holidays_events\": df_holidays_events,\n",
        "    \"oil\": df_oil,\n",
        "    \"train\": df_train\n",
        "}\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    print(\"\\nMissing values in\", name)\n",
        "    print(df.isnull().sum()[df.isnull().sum() > 0])"
      ],
      "metadata": {
        "id": "gqQIgi1NJQYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Data Quality Fixes"
      ],
      "metadata": {
        "id": "mbsFe0KoBKNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Fill onpromotion Nas with False"
      ],
      "metadata": {
        "id": "OJzJ4GbMBPXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill onpromotion NAs with False\n",
        "df_train['onpromotion'] = df_train['onpromotion'].fillna(False).astype(bool)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ow9RSvJNJt1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace False and True by 0 and 1\n",
        "df_train['onpromotion'] = df_train['onpromotion'].apply(lambda x: 1 if x == True else 0)"
      ],
      "metadata": {
        "id": "jsijv4fGd5E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Interpolate Oil Price"
      ],
      "metadata": {
        "id": "pqhmEWLfBW72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Interpolate oil price\n",
        "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].interpolate().fillna(method='bfill')"
      ],
      "metadata": {
        "id": "uRDcIHhTJ1I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Fill Missing Transactions with 0"
      ],
      "metadata": {
        "id": "x348wjgjBby9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing transactions with 0\n",
        "df_transactions[\"transactions\"] = df_transactions[\"transactions\"].fillna(0)"
      ],
      "metadata": {
        "id": "VvjlmdWeJ2xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Adjust negative Sales to 0"
      ],
      "metadata": {
        "id": "eitFywTxBiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for negative sales (returns)\n",
        "negative_sales = df_train[df_train['unit_sales'] < 0]\n",
        "\n",
        "negative_sales.head()  # Viewing negative sales for analysis"
      ],
      "metadata": {
        "id": "0_SdzqONKQqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing negative sales with 0 to reflect returns as non-sales\n",
        "df_train['unit_sales'] = df_train['unit_sales'].apply(lambda x: max(x, 0))"
      ],
      "metadata": {
        "id": "aVWE1hcqBo-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Filling Missing Calender Days"
      ],
      "metadata": {
        "id": "rDScx38pBl1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert date to datetime\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "\n",
        "def fill_calendar(group):\n",
        "    # group contains all rows for ONE (store_nbr, item_nbr) pair\n",
        "    g = group.set_index(\"date\").sort_index()   # use calendar as the index\n",
        "    g = g.asfreq(\"D\", fill_value=0)            # make it daily; add 0 where missing\n",
        "                                                # put the identifiers back (asfreq drops them)\n",
        "    g[\"store_nbr\"] = group[\"store_nbr\"].iloc[0]\n",
        "    g[\"item_nbr\"]  = group[\"item_nbr\"].iloc[0]\n",
        "\n",
        "    return g.reset_index()                     # date back to a normal column\n",
        "\n",
        "df_train = (\n",
        "    df_train\n",
        "    .groupby([\"store_nbr\", \"item_nbr\"], group_keys=False)  # keeps memory low\n",
        "    .apply(fill_calendar)\n",
        ")\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "LhxomMrFiScK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Outlier Handling"
      ],
      "metadata": {
        "id": "C-PUYQMUP099"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Z-Score Calculation and Outlier Detection"
      ],
      "metadata": {
        "id": "JEbog20PQETd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate Z-score for each group (store-item combination)\n",
        "def calculate_store_item_zscore(group):\n",
        "    # Compute mean and standard deviation for each store-item group\n",
        "    mean_sales = group['unit_sales'].mean()\n",
        "    std_sales = group['unit_sales'].std()\n",
        "\n",
        "    # Calculate Z-score for unit_sales (avoiding division by zero for standard deviation), and store it in a new column called z_score\n",
        "    group['z_score'] = (group['unit_sales'] - mean_sales) / (std_sales if std_sales != 0 else 1)\n",
        "    return group\n",
        "\n",
        "# Apply the Z-score calculation to each store-item group, then flatten the index\n",
        "df_train_grouped = df_train.groupby(['store_nbr', 'item_nbr']).apply(calculate_store_item_zscore)\n",
        "df_train_grouped.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Define threshold for outliers (e.g., Z-score > 5)\n",
        "outliers = df_train_grouped[df_train_grouped['z_score'] > 5]\n",
        "\n",
        "# Print summary\n",
        "print(f\"Number of outliers detected: {len(outliers)}\")\n",
        "outliers.head()"
      ],
      "metadata": {
        "id": "gqYOyOsgP5Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape\n"
      ],
      "metadata": {
        "id": "S4HRgnVTqc5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Feature Engineering"
      ],
      "metadata": {
        "id": "RYlfs0FJCAIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Date Splitting"
      ],
      "metadata": {
        "id": "RSfxVYlYGAqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extrat date by year, month, day and day of the week\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "df_train['year'] = df_train['date'].dt.year\n",
        "df_train['month'] = df_train['date'].dt.month\n",
        "df_train['day'] = df_train['date'].dt.day\n",
        "df_train['day_of_week'] = df_train['date'].dt.dayofweek\n",
        "df_train['is_weekend'] = df_train['day_of_week'].isin([5, 6])"
      ],
      "metadata": {
        "id": "95uKkgSbGFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Rolling Means"
      ],
      "metadata": {
        "id": "zp-CGWASGHqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating rolling average of unit_sales\n",
        "# a 7-day average puts weekday and weekend sales on equal footing\n",
        "df_train['unit_sales_7d_avg'] = df_train.groupby(['item_nbr', 'store_nbr'])['unit_sales'].transform(lambda x: x.rolling(window=7).mean())"
      ],
      "metadata": {
        "id": "ONzlPkixGJj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Lag Features"
      ],
      "metadata": {
        "id": "k7oqdmi8GNiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lag features for sales data:\n",
        "# for short-term,weekly and monthly patterns in sales behavior\n",
        "df_train['lag_1'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(1) #short-term\n",
        "df_train['lag_7'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(7) #weekly\n",
        "df_train['lag_30'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(30) #monthly"
      ],
      "metadata": {
        "id": "LawRaAPsG-Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Merge Holidays"
      ],
      "metadata": {
        "id": "fI5aYUdaG_kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the holiday merge in chunks to avoid memory overload\n",
        "chunk_size = 1000000\n",
        "chunks = []\n",
        "\n",
        "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])\n",
        "holidays_dict = df_holidays_events.set_index('date')['type'].to_dict()\n",
        "\n",
        "for i in range(0, len(df_train), chunk_size):\n",
        "    chunk = df_train.iloc[i:i+chunk_size].copy()\n",
        "\n",
        "    # Map holiday information using the dictionary\n",
        "    chunk['holiday_type'] = chunk['date'].map(holidays_dict)\n",
        "    chunk['is_holiday'] = chunk['holiday_type'].notna()\n",
        "\n",
        "    chunks.append(chunk)\n",
        "\n",
        "    # Clear memory\n",
        "    del chunk\n",
        "    gc.collect()\n",
        "\n",
        "# Concatenate all chunks\n",
        "df_train = pd.concat(chunks, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "BvsYgwY44hUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Sales Growth & Change Rate"
      ],
      "metadata": {
        "id": "neT4jYnyHEbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measures momentum (for promotions and seasonality)\n",
        "df_train[\"sales_change_7d\"] = (\n",
        "    df_train.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"]\n",
        "    .transform(lambda x: x.pct_change(periods=7))\n",
        ")"
      ],
      "metadata": {
        "id": "X8rlxMtIHGuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Economics Influence from Oil Prices"
      ],
      "metadata": {
        "id": "EvKe8dQr7GsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge oil to capture macroeconomic effects\n",
        "#rooling oul average to smooth volatility\n",
        "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
        "df_oil[\"rolling_oil_7d\"] = df_oil[\"dcoilwtico\"].rolling(7, min_periods=1).mean()\n",
        "df_train = df_train.merge(df_oil, on=\"date\", how=\"left\")"
      ],
      "metadata": {
        "id": "cIHuEbhiHOAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "yp3wWwGyHRKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "L6OyPH1FHSol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Trend Plot"
      ],
      "metadata": {
        "id": "3c0VHyCpCCFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating total sales by date\n",
        "sales_by_date = df_train.groupby('date')['unit_sales'].sum()\n",
        "\n",
        "# Plotting the time-series\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sales_by_date.index, sales_by_date.values)\n",
        "plt.title('Total Unit Sales Over Time in Guayas state', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=16)\n",
        "plt.ylabel('Unit Sales', fontsize=16)\n",
        "plt.xticks(fontsize=14, rotation=45)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HrDvjwG6kb1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Monthly Heatmap"
      ],
      "metadata": {
        "id": "mrI26XS1l6ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating sales by year and month\n",
        "sales_by_month = df_train.groupby(['year', 'month'])['unit_sales'].sum().unstack()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))  # Increase figure size for better visibility\n",
        "sns.heatmap(\n",
        "    sales_by_month,\n",
        "    cmap='coolwarm',  # Use a diverging colormap for better contrast\n",
        "    linewidths=0.5,  # Add lines between cells for clarity\n",
        "    linecolor='white',  # Use white lines for a cleaner look\n",
        "    cbar_kws={'label': 'Sales Volume'}  # Add a descriptive colorbar label\n",
        ")\n",
        "\n",
        "# Customizing title and axes labels\n",
        "plt.title('Monthly Sales Trends Over Years', fontsize=22, fontweight='bold')\n",
        "plt.xlabel('Month', fontsize=18, labelpad=10)  # Labelpad adds spacing\n",
        "plt.ylabel('Year', fontsize=18, labelpad=10)\n",
        "\n",
        "# Formatting tick labels\n",
        "plt.xticks(fontsize=14, rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z0MnTD7Ul8vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[(df_train['year'] == 2017) & (df_train['month'] == 8) & (df_train['day']>=16)]"
      ],
      "metadata": {
        "id": "m0qAtcFXEKCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Anomalie of low sale trend in the last month (August 2017) occurs  because the sales data ends on 15-08-2017. There are a lack of data for the half month"
      ],
      "metadata": {
        "id": "xwchZs58EJiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Promotion Impact"
      ],
      "metadata": {
        "id": "7IC02tnvmBqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Promotion Impact - Compare sales during promotion vs. non-promotion periods.\n",
        "promo_avg = df_train.groupby(\"onpromotion\")[\"unit_sales\"].mean()\n",
        "promo_avg.plot(kind=\"bar\", title=\"Promotion vs Non-Promotion Sales\")"
      ],
      "metadata": {
        "id": "yJhamQsEmCmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Holiday Impact"
      ],
      "metadata": {
        "id": "JSw83VeVCoBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the impact of holidays exclude day with zero sales like it was at the very beginning\n",
        "df_train_no_zero = df_train[df_train.unit_sales > 0]\n"
      ],
      "metadata": {
        "id": "MPAKWpdkmKxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_no_zero['date'] = pd.to_datetime(df_train_no_zero['date'])\n",
        "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])"
      ],
      "metadata": {
        "id": "E2Itq1G7Ea7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging df_train with zero sales with data with holidays\n",
        "df_train_holiday = pd.merge(df_train_no_zero, df_holidays_events, on='date', how='left')"
      ],
      "metadata": {
        "id": "LQ82lDWZEfYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating sales by holiday and non-holiday\n",
        "holiday_sales = df_train_holiday.groupby('type')['unit_sales'].mean()\n",
        "\n",
        "# Plotting holiday impact\n",
        "plt.figure(figsize=(8,5))\n",
        "holiday_sales.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
        "plt.title('Impact of Holidays on Sales', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('Average Unit Sales', fontsize=16)\n",
        "plt.xlabel('')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0KvM4mIXEfxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impact of Holidays\n",
        "nonzero_sales = df_train[df_train['unit_sales'] > 0]\n",
        "holiday_avg = nonzero_sales.groupby('is_holiday')['unit_sales'].mean()\n",
        "holiday_avg.plot(kind='bar', color='lightgreen', edgecolor='black', title=\"Holiday Impact on Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tzbzYfunEkj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Perishable Share"
      ],
      "metadata": {
        "id": "8d9MTyGfDBcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measures momentum (for promotions and seasonality)\n",
        "df_train[\"sales_change_7d\"] = (\n",
        "    df_train.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"]\n",
        "    .transform(lambda x: x.pct_change(periods=7)))"
      ],
      "metadata": {
        "id": "6eqPRoY3Cp_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Economic Influence from Oil Prices"
      ],
      "metadata": {
        "id": "ge0MNNcKDHxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging df_train with items to get perishable data\n",
        "df_items['perishable']= df_items['perishable'].astype(bool)\n",
        "df_train_items = pd.merge(df_train, df_items, on='item_nbr', how='left')\n",
        "df_train_items.head()"
      ],
      "metadata": {
        "id": "eB1tU6VumLbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_items['perishable'].value_counts()"
      ],
      "metadata": {
        "id": "Un-wiqPqDY6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.Save Prepared Dataset"
      ],
      "metadata": {
        "id": "voXyvOe-86-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter DataFrame by date range\n",
        "min_date = '2014-01-01'\n",
        "max_date = '2014-04-01'\n",
        "\n",
        "df_train_filter = df_train_items[\n",
        "    (df_train_items['date'] >= min_date) &\n",
        "    (df_train_items['date'] < max_date)\n",
        "]\n",
        "\n",
        "# Define Google Drive folder path\n",
        "drive_folder_path = '/content/drive/MyDrive/Time Series Analysis /favorita-grocery-sales-forecasting'\n",
        "\n",
        "# Save the CSV file\n",
        "output_file_path = f\"{drive_folder_path}guayas_dataset.csv\"\n",
        "df_train_filter.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "1XX0tX2w9AnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}