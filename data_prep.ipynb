{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b033d3c",
   "metadata": {},
   "source": [
    "# Time Series Analysis for Guayas Region / Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28301a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Import for Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "#Import standard Libraries \n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792f57e",
   "metadata": {},
   "source": [
    "## 2. Confiq and Loading DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a38d1",
   "metadata": {},
   "source": [
    "### 1. Set Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d1e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Data Path\n",
    "DATA_PATH = Path(\"/Users/lukasfichtner/Documents/Guayana_Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402c7b3",
   "metadata": {},
   "source": [
    "### 2. Loading DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all supporting CSV files\n",
    "df_items = pd.read_csv(DATA_PATH / \"items.csv\")\n",
    "df_stores = pd.read_csv(DATA_PATH / \"stores.csv\")\n",
    "df_oil = pd.read_csv(DATA_PATH / \"oil.csv\")\n",
    "df_holidays_events = pd.read_csv(DATA_PATH / \"holidays_events.csv\")\n",
    "df_transactions = pd.read_csv(DATA_PATH / \"transactions.csv\")\n",
    "df_train = pd.read_csv(DATA_PATH/ \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4e5f7",
   "metadata": {},
   "source": [
    "### 3. Reduce Data by Top 3 Product Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6498cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of items per family and select the top 3 families\n",
    "item_counts = df_items.groupby(\"family\")[\"item_nbr\"].nunique().sort_values(ascending=False)\n",
    "top_families = item_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ceb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only items that belong to the top families\n",
    "items_top = df_items[df_items[\"family\"].isin(top_families.index)]  # use .index (family names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only training data with those items\n",
    "df_train = df_train[df_train['item_nbr'].isin(items_top[\"item_nbr\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f477e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot the top 3 families\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(top_families.index, top_families.values, color='skyblue', edgecolor='black')  # Use .index and .values\n",
    "plt.title(\"Top 3 Families by Number of Items\", fontsize=20, fontweight='bold')\n",
    "plt.xlabel(\"Family\", fontsize=16)\n",
    "plt.ylabel(\"Number of Items\", fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8103c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e092330",
   "metadata": {},
   "source": [
    "### 4. Save df_train as Pickel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c08e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataFrame to drive folder as .pickle file\n",
    "save_path = (DATA_PATH/'df_train_filtered.pkl')\n",
    "df_train.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c534d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_train dataframe from pickle.file to reload without rerunning the chunk loop again\n",
    "df_train = pd.read_pickle(DATA_PATH / \"df_train_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb91ed",
   "metadata": {},
   "source": [
    "## 3. Data Quality Fixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a99b",
   "metadata": {},
   "source": [
    "### 1. Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    \"stores\": df_stores,\n",
    "    \"items\": df_items,\n",
    "    \"transactions\": df_transactions,\n",
    "    \"holidays_events\": df_holidays_events,\n",
    "    \"oil\": df_oil,\n",
    "    \"train\": df_train\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(\"\\nMissing values in\", name)\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16300b94",
   "metadata": {},
   "source": [
    "### 2. Fill Nas Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed36945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill onpromotion NAs with False\n",
    "df_train['onpromotion'] = df_train['onpromotion'].fillna(False).astype(bool)\n",
    "# Replace False and True by 0 and 1\n",
    "df_train['onpromotion'] = df_train['onpromotion'].apply(lambda x: 1 if x == True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Interpolate oil price\n",
    "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].interpolate().fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing transactions with 0\n",
    "df_transactions[\"transactions\"] = df_transactions[\"transactions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc572c6",
   "metadata": {},
   "source": [
    "### 3. Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for negative sales (returns)\n",
    "negative_sales = df_train[df_train['unit_sales'] < 0]\n",
    "negative_sales.head()  # Viewing negative sales for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing negative sales with 0 to reflect returns as non-sales\n",
    "df_train['unit_sales'] = df_train['unit_sales'].apply(lambda x: max(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing calender days\n",
    "\n",
    "# convert date to datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "\n",
    "def fill_calendar(group):\n",
    "    # group contains all rows for ONE (store_nbr, item_nbr) pair\n",
    "    g = group.set_index(\"date\").sort_index()   # use calendar as the index\n",
    "    g = g.asfreq(\"D\", fill_value=0)            # make it daily; add 0 where missing\n",
    "                                                # put the identifiers back (asfreq drops them)\n",
    "    g[\"store_nbr\"] = group[\"store_nbr\"].iloc[0]\n",
    "    g[\"item_nbr\"]  = group[\"item_nbr\"].iloc[0]\n",
    "\n",
    "    return g.reset_index()                     # date back to a normal column\n",
    "\n",
    "df_train = (\n",
    "    df_train\n",
    "    .groupby([\"store_nbr\", \"item_nbr\"], group_keys=False)  # keeps memory low\n",
    "    .apply(fill_calendar)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06254374",
   "metadata": {},
   "source": [
    "## 4. Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aca34b",
   "metadata": {},
   "source": [
    "### 1.Z-Score Calculation and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Z-score for each group (store-item combination)\n",
    "def calculate_store_item_zscore(group):\n",
    "    # Compute mean and standard deviation for each store-item group\n",
    "    mean_sales = group['unit_sales'].mean()\n",
    "    std_sales = group['unit_sales'].std()\n",
    "\n",
    "    # Calculate Z-score for unit_sales (avoiding division by zero for standard deviation), and store it in a new column called z_score\n",
    "    group['z_score'] = (group['unit_sales'] - mean_sales) / (std_sales if std_sales != 0 else 1)\n",
    "    return group\n",
    "\n",
    "# Apply the Z-score calculation to each store-item group, then flatten the index\n",
    "df_train_grouped = df_train.groupby(['store_nbr', 'item_nbr']).apply(calculate_store_item_zscore)\n",
    "df_train_grouped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define threshold for outliers (e.g., Z-score > 5)\n",
    "outliers = df_train_grouped[df_train_grouped['z_score'] > 5]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "outliers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213a520",
   "metadata": {},
   "source": [
    "### 2. Visualization of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f05632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram of Z-scores\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train_grouped['z_score'].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(5, color='red', linestyle='--', label='Outlier Threshold (Z=5)')\n",
    "plt.xlabel('Z-score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Z-scores')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2802d1",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc47b94",
   "metadata": {},
   "source": [
    "### 1. Date Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrat date by year, month, day and day of the week\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_train['year'] = df_train['date'].dt.year\n",
    "df_train['month'] = df_train['date'].dt.month\n",
    "df_train['day'] = df_train['date'].dt.day\n",
    "df_train['day_of_week'] = df_train['date'].dt.dayofweek\n",
    "df_train['is_weekend'] = df_train['day_of_week'].isin([5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154cb51",
   "metadata": {},
   "source": [
    "### 2. Rolling Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating rolling average of unit_sales\n",
    "# a 7-day average puts weekday and weekend sales on equal footing\n",
    "df_train['_avg'] = df_train.groupby(['item_nbr', 'store_nbr'])['unit_sales'].transform(lambda x: x.rolling(window=7).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e213e",
   "metadata": {},
   "source": [
    "### 3. Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007eb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for sales data:\n",
    "# for short-term,weekly and monthly patterns in sales behavior\n",
    "df_train['lag_1'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(1) #short-term\n",
    "df_train['lag_7'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(7) #weekly\n",
    "df_train['lag_30'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(30) #monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5664309",
   "metadata": {},
   "source": [
    "### 4. Merge Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b63734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the holiday merge in chunks to avoid memory overload\n",
    "chunk_size = 1000000\n",
    "chunks = []\n",
    "\n",
    "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])\n",
    "holidays_dict = df_holidays_events.set_index('date')['type'].to_dict()\n",
    "\n",
    "for i in range(0, len(df_train), chunk_size):\n",
    "    chunk = df_train.iloc[i:i+chunk_size].copy()\n",
    "\n",
    "    # Map holiday information using the dictionary\n",
    "    chunk['holiday_type'] = chunk['date'].map(holidays_dict)\n",
    "    chunk['is_holiday'] = chunk['holiday_type'].notna()\n",
    "\n",
    "    chunks.append(chunk)\n",
    "\n",
    "    # Clear memory\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all chunks\n",
    "df_train = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df93a0",
   "metadata": {},
   "source": [
    "### 5. Sales Growth & Change Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures momentum (for promotions and seasonality)\n",
    "df_train[\"sales_change_7d\"] = (\n",
    "    df_train.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"]\n",
    "    .transform(lambda x: x.pct_change(periods=7))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f98f9",
   "metadata": {},
   "source": [
    "### 6.Economics Influence from Oil Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge oil to capture macroeconomic effects\n",
    "# rooling oul average to smooth volatility\n",
    "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "df_oil[\"rolling_oil_7d\"] = df_oil[\"dcoilwtico\"].rolling(7, min_periods=1).mean()\n",
    "df_train = df_train.merge(df_oil, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e28a9",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00978049",
   "metadata": {},
   "source": [
    "### 1. Trend Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6397d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating total sales by date\n",
    "sales_by_date = df_train.groupby('date')['unit_sales'].sum()\n",
    "\n",
    "# Plotting the time-series\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(sales_by_date.index, sales_by_date.values)\n",
    "plt.title('Total Unit Sales Over Time in Guayas state', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.ylabel('Unit Sales', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdb0eb",
   "metadata": {},
   "source": [
    "### 2. Monthly Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6456d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating sales by year and month\n",
    "sales_by_month = df_train.groupby(['year', 'month'])['unit_sales'].sum().unstack()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))  # Increase figure size for better visibility\n",
    "sn.heatmap(\n",
    "    sales_by_month,\n",
    "    cmap='coolwarm',  # Use a diverging colormap for better contrast\n",
    "    linewidths=0.5,  # Add lines between cells for clarity\n",
    "    linecolor='white',  # Use white lines for a cleaner look\n",
    "    cbar_kws={'label': 'Sales Volume'}  # Add a descriptive colorbar label\n",
    ")\n",
    "\n",
    "# Customizing title and axes labels\n",
    "plt.title('Monthly Sales Trends Over Years', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Month', fontsize=18, labelpad=10)  # Labelpad adds spacing\n",
    "plt.ylabel('Year', fontsize=18, labelpad=10)\n",
    "\n",
    "# Formatting tick labels\n",
    "plt.xticks(fontsize=14, rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cc51f",
   "metadata": {},
   "source": [
    "The Anomalie of low sale trend in the last month (August 2017) occurs  because the sales data ends on 15-08-2017. There are a lack of data for the half month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99897e5d",
   "metadata": {},
   "source": [
    "### 3. Promotion Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Promotion Impact - Compare sales during promotion vs. non-promotion periods.\n",
    "promo_avg = df_train.groupby(\"onpromotion\")[\"unit_sales\"].mean()\n",
    "promo_avg.plot(kind=\"bar\", title=\"Promotion vs Non-Promotion Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be389500",
   "metadata": {},
   "source": [
    "### 4.Holiday Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the impact of holidays exclude day with zero sales like it was at the very beginning\n",
    "df_train_no_zero = df_train[df_train.unit_sales > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e452782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_no_zero['date'] = pd.to_datetime(df_train_no_zero['date'])\n",
    "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355443b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merging df_train with zero sales with data with holidays\n",
    "df_train_holiday = pd.merge(df_train_no_zero, df_holidays_events, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b789fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating sales by holiday and non-holiday\n",
    "holiday_sales = df_train_holiday.groupby('type')['unit_sales'].mean()\n",
    "\n",
    "# Plotting holiday impact\n",
    "plt.figure(figsize=(8,5))\n",
    "holiday_sales.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
    "plt.title('Impact of Holidays on Sales', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Average Unit Sales', fontsize=16)\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2522dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact of Holidays\n",
    "nonzero_sales = df_train[df_train['unit_sales'] > 0]\n",
    "holiday_avg = nonzero_sales.groupby('is_holiday')['unit_sales'].mean()\n",
    "holiday_avg.plot(kind='bar', color='lightgreen', edgecolor='black', title=\"Holiday Impact on Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e581bd7",
   "metadata": {},
   "source": [
    "### 5. Perishables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging df_train with items to get perishable data\n",
    "df_items['perishable']= df_items['perishable'].astype(bool)\n",
    "df_train_items = pd.merge(df_train, df_items, on='item_nbr', how='left')\n",
    "df_train_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4e1c3",
   "metadata": {},
   "source": [
    "## 7. Save Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CSV file\n",
    "output_file_path = f\"{DATA_PATH}guayas_dataset.csv\"\n",
    "df_train_items.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
